# 출력 값이 입력 값의 상수배만큼 변하는 함수를 선형 함수라고 정의. 선형 함수는 직선 1개로 나타낼 수 있음
# 비선형 함수는 선형이 아닌 함수로, 직선 1개만으로 나타낼 수 없고 곡선 또는 여러 개의 선으로 나타낼 수 있음
# 활성화 함수로 비선형 함수를 사용해야 은닉층이 존재하는 다층 네트워크를 구성할 수 있음

# 계단 함수, 시그모이드 함수 외의 또다른 활성화 함수로 ReLU 함수 존재
# ReLU 함수 : 입력이 0을 넘으면 입력값, 입력이 0 미만이면 0 출력

import numpy as np

# relu 함수
def relu(x):
    # 0과 x 중에서 큰 값을 리턴 (x가 0보다 작으면 0, x가 0 이상이면 x 출력)
    return np.maximum(0,x)

# reLU 함수의 테스트 결과 출력
print(relu(-0.5))
print(relu(0.2))