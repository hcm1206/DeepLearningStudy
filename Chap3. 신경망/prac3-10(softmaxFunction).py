# 신경망의 출력층 설계

# 기계학습 문제는 '분류'와 '회귀' 두 가지로 구분
# 분류 : 데이터가 어떤 클래스에 속하는지 결정하는 문제 (ex : 이 사진 속 인물의 성별은 무엇인가?)
# 회귀 : 입력 데이터에서 연속적인 수치 중 어떤 수치인지를 예측하는 문제 (ex : 이 사진 속 인물의 몸무게는 몇 kg인가?)

# 회귀 문제에서는 출력층에 항등함수를 사용
# 분류 문제에서는 출력층에 소프트맥스 함수를 사용

# 소프트맥스 함수의 식 : k번쨰 출력값은 k번째 입력신호(계산 값)을 모든 입력신호(계산 값)의 자연상수 지수 함수값을 더한 값으로 나눈 것

import numpy as np

# 입력값으로 사용할 뉴런 3개의 값을 넘파이 배열로 a에 저장
a = np.array([0.3, 2.9, 4.0])

# a 배열에 저장된 뉴런 값들을 자연상수의 지수로 하는 지수함수 계산 값을 exp_a에 저장
exp_a = np.exp(a)
print(exp_a)

# exp_a에 저장된 모든 뉴런값들의 합을 sum_exp_a에 저장
sum_exp_a = np.sum(exp_a)
print(sum_exp_a)

# exp_a에 저장된 값(배열에 저장된 지수함수 계산 값)을 sum_exp_a(exp_a 배열의 모든 지수함수 계산 값의 합)으로 나눈 값을 y에 배열로 저장
y = exp_a / sum_exp_a
print(y)

print()


# 지수함수를 계산할 때는 컴퓨터가 일반적으로 계산하는 범위를 넘어서는 큰 수를 다루기 때문에 오버플로우 발생 가능성이 높음
# 따라서 위의 식을 그대로 사용하면 연산 결과가 불안정해지거나 정상적으로 동작하지 않을 수 있음

# a에 일정량 이상의 큰 수를 저장하여 계산하면 오버플로우가 발생하여 비정상적인 값을 출력
a = np.array([1010, 1000, 990])
print(np.exp(a) / np.sum(np.exp(a)))

print()

# 하지만 a 배열에서 최댓값을 구한 후 배열의 원소(뉴런) 값을 최대값으로 뺀 후 계산하면 동일한 결과로 정상적인 값 출력

# a 배열의 최댓값을 c에 저장
c = np.max(a)
# a 배열의 모든 원소(뉴런) 값을 c(최댓값)로 뺀 배열 결과 출력
print(a - c)

# a 배열의 모든 원소(뉴런) 값을 c(최댓값)로 뺀 배열을 소프트맥스 함수에 입력한 결과 값 출력(정상 출력)
print(np.exp(a-c)/np.sum(np.exp(a-c)))


print()

# 오버플로 대책을 적용한 소프트맥수 함수 구현
def softmax(a):
    c = max(a)
    # 지수함수 계산 시 배열의 뉴런 값을 최댓값으로 뺀 값으로 계산
    exp_a = np.exp(a-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a/sum_exp_a
    return y


print()

# 소프트맥스 함수의 출력 값은 0과 1사이의 실수이고, 배열의 모든 출력 값들의 합은 1
# 이를 통해 각 뉴런(클래스)들을 확률로 해석하여 통계적으로 대응 가능

a = np.array([0.3, 2.9, 4.0])
y = softmax(a)
print(y)
# 소프트맥스 출력 값들의 합은 1
print(sum(y))

print()

# 소프트맥스 함수의 입력값과 출력값의 대소관계는 항상 같음 (지수 함수 y = exp(x)가 단조 증가 함수이기 때문)
# 따라서 신경망으로 분류 시 출력층의 소프트맥스 함수 생략 가능
# 기계학습의 문제 풀이는 모델을 학습하는 '학습', 학습한 모델로 데이터를 분류하는 '추론'과정 존재
# '추론' 단계에서는 소프르맥스 함수 생략 가능, '학습' 단계에서는 출력층에서 소프트맥스 함수 사용