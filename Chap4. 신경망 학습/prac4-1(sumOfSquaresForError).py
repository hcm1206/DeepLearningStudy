# 신경망에서는 어떤 지표를 토대로 최적의 매개변수 값을 탐색
# 이 지표를 '손실 함수(Loss Function)'라 하며 일반적으로 오차제곱합과 교차 엔트로피 오차를 사용
# 손실 함수의 값이 클 수록 신경망 성능이 좋지 않다는 뜻, 손실 함수의 값을 줄여나가는 과정이 신경망 학습

# 손실 함수로 가장 많이 쓰이는 것이 '오차제곱합(sum of squares for error)'
# yk(신경망의 출력(신경망이 추정한 값))에서 tk(정답 레이블(실제 정답 값)을 빼서 제곱한 값들의 합 × 0.5

# yk와 tk 모두 배열, 여기서 k는 차원 수를 의미
# Chap3의 손글씨 인식에서 yk는 신경망이 추정한 각각의 글씨 수치(확률), tk는 원-핫 인코딩 방식의 정답 데이터


import numpy as np

# 신경망이 이 입력값은 신경망 추론 결과(소프트맥스 함수의 출력)를 통해 가장 가능성이 높은 값은 출력층의 2번 인덱스(숫자 2)의 값이라고 추정
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
# 실제 정답은 2번 인덱스(숫자 2)
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
 
# 오차제곱합(각 원소의 출력과 정답 레이블의 차를 제곱한 총합)을 함수로 구현
def sum_squares_error(y,t):
    return 0.5 * np.sum((y-t)**2)

# 정답 레이블 (숫자 2가 정답)
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
# 입력 예시 1 : 숫자가 2일 확률이 가장 높다고 추정
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
# 오차제곱합 함수에 입력 (배열 계산을 위해 넘파이 배열화 작업 필요)
print(sum_squares_error(np.array(y),np.array(t)))

# 입력 에시 2 : 숫자가 7일 확률이 가장 높다고 추정
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
# 오차제곱합 함수에 입력
print(sum_squares_error(np.array(y),np.array(t)))


# 입력 예시 1의 오차제곱합 결과 값이 입력 예시 2의 오차제곱합 결과 값보다 작음
# 입력 예시 1의 추정이 더 정확하다고 볼 수 있음