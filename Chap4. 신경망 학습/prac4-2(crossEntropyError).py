# 교차 엔트로피 오차(Cross Entropy Error, CEE)도 신경망 학습에 많이 사용되는 또다른 손실 함수
# 결과값은 tk * logyk의 합에 음수 기호를 붙인 것
# yk는 신경망의 추정 값 출력, tk는 실제 정답 레이블(원-핫 인코딩, 정답인 원소만 1이고 나머지는 0인 배열), log는 밑이 e인 자연로그
# 정답으로 추정하지 않은 원소는 결과값에 영향이 없고 정답으로 추정한 원소만 결과값에 영향을 줌
# 결과 값(오차)가 클 수록 부정확하다는 뜻

import numpy as np

def cross_entropy_error(y, t):
    # 계산 오류 방지를 위한 아주 작은 값
    delta = 1e-7
    return -np.sum(t * np.log(y+delta))

# np.log() 함수에 0을 입력하면 마이너스 무한대가 되어 계산 진행이 불가하므로 계산 값에 영향이 미미할 정도의 아주 작은 값을 임의로 더해줌

# 실제 정답이 2번 인덱스인 레이블
t = [0,0,1,0,0,0,0,0,0,0]
# 정답을 2번 인덱스로 추정한 신경망 추론 출력값
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]

# 정답 레이블(정답 : 2)와 신경망 출력값(추론 : 2)를 교차 엔트로피 오차 함수에 입력한 결과 출력
print(cross_entropy_error(np.array(y), np.array(t)))

# 정답을 7번 인덱스로 추정한 신경망 추론 출력값
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]

# 정답 레이블(정답 : 2)와 신경망 출력값(추론 : 7)을 교차 엔트로피 오차 함수에 입력한 결과 출력
print(cross_entropy_error(np.array(y), np.array(t)))


# 실제 정답이 2일 때 정답을 2로 추론한 결과 값보다 정답을 7로 추론한 결과 값(오차)가 더 큼
# 정답을 2로 추론한 결과값이 오차가 더 작으므로 이 추정이 정답일 가능성이 더 높다고 판단